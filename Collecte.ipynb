{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8deabc7-3ace-4316-a9ed-485543a5fa72",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    font-family: 'Georgia', serif;\n",
    "    color: #2c2c2c;\n",
    "    text-align: center;\n",
    "    padding: 20px 0 30px 0;\n",
    "\">\n",
    "\n",
    "  <h2 style=\"\n",
    "      font-size: 26px;\n",
    "      color: #ad1457;\n",
    "      font-weight: bold;\n",
    "      letter-spacing: 0.5px;\n",
    "      border-bottom: 2px solid #f48fb1;\n",
    "      padding-bottom: 6px;\n",
    "      margin: 0 auto 10px auto;\n",
    "      width: fit-content;\n",
    "      min-width: 300px;\n",
    "  \">\n",
    "    Bias Correction of Climate Projection Data\n",
    "  </h2>\n",
    "\n",
    "  <p style=\"font-size: 15px; margin: 8px 0 2px 0;\">\n",
    "    <strong>Hajar BADRAOUI</strong> ‚Äî Data Analyst Intern, ARVALIS (2025)\n",
    "  </p>\n",
    "\n",
    "  <p style=\"font-size: 14px; margin: 2px 0;\">\n",
    "    M2 ‚Äî Statistiques Appliqu√©es & Analyse D√©cisionnelle,<br>\n",
    "    Universit√© de Caen Normandie, France\n",
    "  </p>\n",
    "\n",
    "  <p style=\"font-size: 14px; margin: 10px 0 0 0;\">\n",
    "    Projet : Correction de biais des mod√®les climatiques (SAFRAN / EURO-CORDEX)\n",
    "  </p>\n",
    "\n",
    "  <p style=\"font-size: 14px; margin-top: 10px;\">\n",
    "    üìß <a href=\"mailto:hajar.badraoui01@gmail.com\" style=\"color: #ad1457; text-decoration: none;\">\n",
    "      hajar.badraoui01@gmail.com\n",
    "    </a>\n",
    "  </p>\n",
    "\n",
    "  <p style=\"font-size: 14px; margin-top: 5px;\">\n",
    "    üîó <a href=\"https://github.com/badraouihajar\" target=\"_blank\" style=\"color: #ad1457; text-decoration: none;\">\n",
    "      github.com/badraouihajar\n",
    "    </a>\n",
    "  </p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3193e1a9-6fd1-40f1-b3df-f2f9cee0c2a5",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background-color: #fff;\n",
    "    padding: 26px 30px 18px 30px;\n",
    "    border-radius: 12px;\n",
    "    box-shadow: 2px 2px 8px rgba(0,0,0,0.06);\n",
    "    font-family: 'Georgia', serif;\n",
    "    color: #2c2c2c;\n",
    "    margin: 12px 0 20px 0;\n",
    "\">\n",
    "\n",
    "<h3 style=\"margin-top:0; margin-bottom:18px; color:#d81b60;\">\n",
    "  Introduction ‚Äì Donn√©es & Mod√®les climatiques\n",
    "</h3>\n",
    "<p>\n",
    "Tout d'abord, on commence par t√©l√©charger et pr√©traiter les donn√©es des mod√®les climatiques dans le but de corriger les biais.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Pour appliquer une m√©thode de correction de biais, il est n√©cessaire de disposer de <b>trois ensembles de donn√©es</b>‚ÄØ:\n",
    "<ul style=\"margin-top:8px;\">\n",
    "  <li><b>Donn√©es d'observation ou de r√©analyse</b></li>\n",
    "  <li><b>Donn√©es historiques issues des mod√®les climatiques</b> (m√™me p√©riode que les observations)</li>\n",
    "  <li><b>Donn√©es des mod√®les climatiques pour une p√©riode future</b></li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Dans cette √©tude, nous allons travailler avec des <b>mod√®les climatiques EURO-CORDEX</b> afin d'analyser les √©volutions climatiques pass√©es et futures selon le sc√©nario <b>RCP 4.5</b>.\n",
    "</p>\n",
    "\n",
    "<p style=\"margin-bottom:4px;\"><b>Mod√®les climatiques utilis√©s</b> :</p>\n",
    "<ul>\n",
    "  <li>Mod√®le global <b>CNRM-CERFACS-CM5</b> (France) / mod√®le r√©gional <b>CNRM-ALADIN63</b> (France)</li>\n",
    "  <li>Mod√®le global <b>CNRM-CERFACS-CM5</b> (France) / mod√®le r√©gional <b>KNMI-RACMO22E</b> (Pays-Bas)</li>\n",
    "  <li>Mod√®le global <b>ICHEC-EC-EARTH</b> (Irlande) / mod√®le r√©gional <b>KNMI-RACMO22E</b> (Pays-Bas)</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"margin-bottom:4px;\"><b>P√©riodes et sc√©narios √©tudi√©s</b> :</p>\n",
    "<ul>\n",
    "  <li><b>P√©riode historique</b>‚ÄØ: 1976 ‚Äì 2005</li>\n",
    "  <li><b>Sc√©nario RCP 4.5</b>‚ÄØ: 2006 ‚Äì 2100 (sc√©nario mod√©r√© d‚Äô√©missions)</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"margin-bottom:4px;\"><b>Donn√©es de r√©analyse</b> :</p>\n",
    "<ul>\n",
    "  <li><b>SAFRAN</b>‚ÄØ: donn√©es de r√©analyse utilis√©es comme r√©f√©rence pour valider les simulations climatiques.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f79a6-692b-4c0e-bed4-1fbf3f97960e",
   "metadata": {},
   "source": [
    "le package Iris est un outil Python sp√©cialis√© pour la lecture, manipulation et analyse des donn√©es climatologiques et m√©t√©orologiques au format NetCDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0d08f-4091-4ff0-8e0e-e9d7e1211c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scitools-iris   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec10e8bf-4fa8-4120-8662-718a54be9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import iris\n",
    "import xarray\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86863e0d-5474-444d-97bb-965de2f91519",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: 'Alice', serif; color: #2471A3; font-size: 2em; font-weight: 700;\">\n",
    "  √âtape 01 : Collecte des donn√©es\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40f84e-65a2-46a0-9fe8-520eb7a911ff",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2980b9;;\">1.1 T√©l√©chargement des donn√©es du mod√®le climatique</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf44558-2262-45c5-ae48-1dd582f1c54f",
   "metadata": {},
   "source": [
    "Pour demander des donn√©es climatiques √† partir du Climate Data Store (CDS), nous utiliserons l‚ÄôAPI CDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ae205-76a0-42f4-9a49-56dda1c81194",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cdsapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439a9b14-df65-406f-b361-873ac8cfba2f",
   "metadata": {},
   "source": [
    "Dans l‚Äôensemble des scripts pr√©sent√©s, le traitement est illustr√© avec le mod√®le r√©gional ALADIN (CNRM-ALADIN63), mais la m√™me d√©marche peut √™tre appliqu√©e aux autres mod√®les climatiques utilis√©s dans cette √©tude (ex‚ÄØ: KNMI-RACMO22E, ICHEC-EC-EARTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee9a4d-d995-4626-8934-e8a2f458d835",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#6A5ACD;\">a. T√©l√©chargement des donn√©es du mod√®le climatique historique</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "557f4e98-6313-47e9-a524-b347f6350aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import urllib3\n",
    "urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5ca8c-9756-41fe-99f5-85fee5505f77",
   "metadata": {},
   "source": [
    "Le code ci-dessous permet de t√©l√©charger les fichiers `.zip` directement depuis le site [Copernicus CDS](https://cds.climate.copernicus.eu/datasets/projections-cordex-domains-single-levels?tab=download) pour le jeu de donn√©es EURO-CORDEX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e75e1c-36c7-4fc4-8316-6cd7ee46d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le code n'est pas √† modifier, **sauf** pour les √©l√©ments suivants :\n",
    "# === Les seules choses √† changer sont :\n",
    "#   - gcm_model    : nom du mod√®le global ( cnrm_cerfacs_cm5  , ichec_ec_earth  )\n",
    "#   - rcm_model   :  nom du mod√®le r√©gional (knmi_racmo22e ,  knmi_racmo22e )\n",
    "#   - experiment  : nom du sc√©nario (ex : 'historical', 'rcp_4_5', 'rcp_8_5') (dans ce code est d√©di√© que pour hist, sinon on doit changer les ann√©es)\n",
    "# A RETENIR le dossier destination folder : le dossier qui va contenir les fichiers netcf du mod√®le historique\n",
    "# Tout le reste du code doit rester inchang√©.\n",
    "\n",
    "\n",
    "import cdsapi\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    " \n",
    "#D√©sactiver les widgets Jupyter (√©vite le bug de blocage)\n",
    "sys.modules[\"ipywidgets\"] = None\n",
    " \n",
    "#Dossier de destination\n",
    "destination_folder =\"D:/cordex_data/donnees_historiques\"        # √† VOIR \n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    " \n",
    "#Initialisation de l'API\n",
    "client = cdsapi.Client()\n",
    " \n",
    "#Param√®tres du dataset\n",
    "dataset = \"projections-cordex-domains-single-levels\"\n",
    "experiment = \"historical\"                                 # √† MODIFIER\n",
    "gcm_model = \"cnrm_cerfacs_cm5\"                            # √† MODIFIER\n",
    "rcm_model = \"cnrm_aladin63\"                               # √† MODIFIER\n",
    "ensemble_member = \"r1i1p1\"\n",
    "horizontal_resolution = \"0_11_degree_x_0_11_degree\"\n",
    "temporal_resolution = \"daily_mean\"\n",
    " \n",
    "#Variables climatiques\n",
    "variables = [\n",
    "    \"2m_relative_humidity\",\n",
    "    \"10m_wind_speed\",\n",
    "    \"maximum_2m_temperature_in_the_last_24_hours\",\n",
    "    \"minimum_2m_temperature_in_the_last_24_hours\",\n",
    "    \"mean_precipitation_flux\",\n",
    "    \"surface_solar_radiation_downwards\"\n",
    "]\n",
    " \n",
    "#P√©riodes √† t√©l√©charger\n",
    "periods = [\n",
    "    (\"1951\", \"1955\"), (\"1956\", \"1960\"), (\"1961\", \"1965\"),\n",
    "    (\"1966\", \"1970\"), (\"1971\", \"1975\"), (\"1976\", \"1980\"),\n",
    "    (\"1981\", \"1985\"), (\"1986\", \"1990\"), (\"1991\", \"1995\"),\n",
    "    (\"1996\", \"2000\"), (\"2001\", \"2005\"),\n",
    "]\n",
    " \n",
    "#T√©l√©chargement par p√©riode\n",
    "for i, (start_year, end_year) in enumerate(periods):\n",
    "    filename = f\"{destination_folder}/cordex_{experiment}_{gcm_model}_{rcm_model}_{start_year}_{end_year}.zip\"\n",
    " \n",
    "    print(f\"\\n Bloc {i+1}/{len(periods)} : {start_year}-{end_year}\")\n",
    " \n",
    "    #  Ne pas re-t√©l√©charger si d√©j√† pr√©sent\n",
    "    if os.path.exists(filename) and os.path.getsize(filename) > 0:\n",
    "        print(f\" D√©j√† t√©l√©charg√© : {filename}\")\n",
    "        continue\n",
    " \n",
    "    print(f\" T√©l√©chargement en cours...\")\n",
    " \n",
    "    request = {\n",
    "        \"domain\": \"europe\",\n",
    "        \"experiment\": experiment,\n",
    "        \"horizontal_resolution\": horizontal_resolution,\n",
    "        \"temporal_resolution\": temporal_resolution,\n",
    "        \"variable\": variables,\n",
    "        \"gcm_model\": gcm_model,\n",
    "        \"rcm_model\": rcm_model,\n",
    "        \"ensemble_member\": ensemble_member,\n",
    "        \"start_year\": [start_year],\n",
    "        \"end_year\": [end_year],\n",
    "        \"format\": \"zip\" \n",
    "    }\n",
    " \n",
    "    try:\n",
    "        result = client.retrieve(dataset, request)\n",
    "        print(\" T√©l√©chargement termin√©. Sauvegarde du fichier (en mode console)...\")\n",
    "        path = result.download(filename)\n",
    "        print(f\" Fichier enregistr√© : {path}\")\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\" Erreur pour {start_year}-{end_year} : {e}\")\n",
    " \n",
    "print(\"\\n Tous les blocs ont √©t√© trait√©s avec succ√®s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b58b9-3002-4d28-b2c4-fe53eb43fdda",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#6A5ACD;\">b. Extraction g√©ographique : Zone France</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c2b4f3-222d-4e10-9eab-7a9b7d66293b",
   "metadata": {},
   "source": [
    "Cette section pr√©sente un script permettant d‚Äôextraire automatiquement **les donn√©es climatiques historiques du mod√®le CNRM-CERFACS-CM5-CNRM-ALADIN63**, en nous concentrant uniquement sur le **territoire fran√ßais** (m√™me chose pour les autres mod√®les).\n",
    "\n",
    "L‚Äôobjectif est de parcourir l‚Äôensemble des fichiers NetCDF disponibles, de s√©lectionner la zone correspondant aux limites g√©ographiques de la France, puis de sauvegarder ces sous-ensembles pour une analyse cibl√©e √† l‚Äô√©chelle nationale.\n",
    "\n",
    "Le script ci-dessous permet d‚Äôautomatiser cette extraction pour tous les fichiers du r√©pertoire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e51f06-ad98-41ef-944b-a64de6333aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le code doit rester inchang√©, √† l‚Äôexception de la variable `input_folder`.\n",
    "#  Il faut sp√©cifier un sous-dossier dans le dossier des donn√©es historiques du mod√®le,\n",
    "# nomm√© selon le mod√®le utilis√©.\n",
    "# Exemple¬†: si le dossier principal est `donnees_historiques` et le mod√®le est\n",
    "# `CNRM-CERFACS-CM5_CNRM-ALADIN63`, alors :\n",
    "#     input_folder = 'donnees_historiques/CNRM-CERFACS-CM5_CNRM-ALADIN63'\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Param√®tres\n",
    "input_folder = \"D:/cordex_data/donnees_historiques/CNRM-CERFACS-CM5_CNRM-ALADIN63\"    # changer CNRM-CERFACS-CM5_CNRM-ALADIN63 par Mod√®le qu'on veut\n",
    "output_folder = os.path.join(input_folder, \"france_extrait\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Limites g√©ographiques de la France\n",
    "lat_min, lat_max = 41, 51\n",
    "lon_min, lon_max = -5, 10\n",
    "\n",
    "# Boucle sur tous les fichiers NetCDF du dossier\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".nc\"):\n",
    "        path = os.path.join(input_folder, file)\n",
    "        print(f\"\\n Traitement : {file}\")\n",
    "        try:\n",
    "            ds = xr.open_dataset(path)\n",
    "\n",
    "            # Lecture des coordonn√©es lat/lon\n",
    "            lat2d = ds[\"lat\"]\n",
    "            lon2d = ds[\"lon\"]\n",
    "\n",
    "            # Masque pour la France\n",
    "            mask = (lat2d >= lat_min) & (lat2d <= lat_max) & (lon2d >= lon_min) & (lon2d <= lon_max)\n",
    "            iy, ix = np.where(mask)\n",
    "\n",
    "            if iy.size == 0 or ix.size == 0:\n",
    "                print(\" Aucun point dans la zone France.\")\n",
    "                continue\n",
    "\n",
    "            ymin, ymax = iy.min(), iy.max()\n",
    "            xmin, xmax = ix.min(), ix.max()\n",
    "\n",
    "            # Identifier la variable principale (hors lat/lon/time_bounds)\n",
    "            var_names = [v for v in ds.data_vars if v not in ['lat', 'lon', 'time_bounds']]\n",
    "            if not var_names:\n",
    "                print(\" Pas de variable principale trouv√©e.\")\n",
    "                continue\n",
    "\n",
    "            var_name = var_names[0]\n",
    "            print(f\" Variable d√©tect√©e : {var_name}\")\n",
    "\n",
    "            # Extraire les donn√©es pour la France\n",
    "            data_subset = ds[var_name].isel(x=slice(xmin, xmax + 1), y=slice(ymin, ymax + 1))\n",
    "            lat_subset = lat2d.isel(y=slice(ymin, ymax + 1), x=slice(xmin, xmax + 1))\n",
    "            lon_subset = lon2d.isel(y=slice(ymin, ymax + 1), x=slice(xmin, xmax + 1))\n",
    "\n",
    "            # R√©assigner les coordonn√©es\n",
    "            data_subset = data_subset.assign_coords(lat=lat_subset, lon=lon_subset)\n",
    "\n",
    "            # Sauvegarde dans le dossier 'france_extrait'\n",
    "            output_path = os.path.join(output_folder, f\"{var_name}_FR_{file}\")\n",
    "            data_subset.to_netcdf(output_path)\n",
    "            print(f\" Sauvegard√© : {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Erreur pour {file} : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f27e73-b617-475b-8e8e-6ea2703147c3",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#6A5ACD;\">c. T√©l√©chargement des donn√©es du mod√®le climatique future</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc54277-c86d-4913-878e-d69e043ba7d4",
   "metadata": {},
   "source": [
    "Nous appliquons les m√™mes configurations de mod√®les que pr√©c√©demment, cette fois dans le cadre du sc√©nario **RCP 4.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0cae3-cad0-4aa1-b4aa-cadf6fc413da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le code n'est pas √† modifier, **sauf** pour les √©l√©ments suivants :\n",
    "# === Les seules choses √† changer sont :\n",
    "#   - gcm_model    : nom du mod√®le global ( cnrm_cerfacs_cm5  , ichec_ec_earth  )\n",
    "#   - rcm_model   :  nom du mod√®le r√©gional (knmi_racmo22e ,  knmi_racmo22e )\n",
    "#   - experiment  : nom du sc√©nario (ex : 'rcp_4_5', 'rcp_8_5') \n",
    "# A RETENIR le dossier destination folder : le dossier qui va contenir les fichiers netcf du mod√®le futur\n",
    "# Tout le reste du code doit rester inchang√©.\n",
    "\n",
    "\n",
    "\n",
    "import cdsapi\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.modules[\"ipywidgets\"] = None\n",
    "destination_folder = \"D:/cordex_data/donnees_futures\"      # √† VOIR\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "client = cdsapi.Client()\n",
    "\n",
    "\n",
    "dataset = \"projections-cordex-domains-single-levels\"\n",
    "experiment = \"rcp_4_5\"                                             # √† MODIFIER\n",
    "gcm_model = \"cnrm_cerfacs_cm5\"                                    # √† MODIFIER                              \n",
    "rcm_model = \"cnrm_aladin63\"                                       # √† MODIFIER\n",
    "ensemble_member = \"r1i1p1\"\n",
    "horizontal_resolution = \"0_11_degree_x_0_11_degree\"\n",
    "temporal_resolution = \"daily_mean\"\n",
    "\n",
    "\n",
    "variables = [\n",
    "    \"2m_relative_humidity\",\n",
    "    \"10m_wind_speed\",\n",
    "    \"maximum_2m_temperature_in_the_last_24_hours\",\n",
    "    \"minimum_2m_temperature_in_the_last_24_hours\",\n",
    "    \"mean_precipitation_flux\",\n",
    "    \"surface_solar_radiation_downwards\"\n",
    "]\n",
    "\n",
    "\n",
    "periods = [\n",
    "    (\"2006\", \"2010\"), (\"2011\", \"2015\"), (\"2016\", \"2020\"),\n",
    "    (\"2021\", \"2025\"), (\"2026\", \"2030\"), (\"2031\", \"2035\"),\n",
    "    (\"2036\", \"2040\"), (\"2041\", \"2045\"), (\"2046\", \"2050\"),\n",
    "    (\"2051\", \"2055\"), (\"2056\", \"2060\"), (\"2061\", \"2065\"),\n",
    "    (\"2066\", \"2070\"), (\"2071\", \"2075\"), (\"2076\", \"2080\"),\n",
    "    (\"2081\", \"2085\"), (\"2086\", \"2090\"), (\"2091\", \"2095\"),\n",
    "    (\"2096\", \"2100\"),\n",
    "]\n",
    "\n",
    "\n",
    "for i, (start_year, end_year) in enumerate(periods):\n",
    "    filename = f\"{destination_folder}/cordex_{experiment}_{gcm_model}_{rcm_model}_{start_year}_{end_year}.zip\"\n",
    "    print(f\"\\nBloc {i+1}/{len(periods)} : {start_year}-{end_year}\")\n",
    "\n",
    "   \n",
    "    if os.path.exists(filename) and os.path.getsize(filename) > 0:\n",
    "        print(f\"D√©j√† t√©l√©charg√© : {filename}\")\n",
    "        continue\n",
    "\n",
    "    print(\"T√©l√©chargement en cours...\")\n",
    "\n",
    "    request = {\n",
    "        \"domain\": \"europe\",\n",
    "        \"experiment\": experiment,\n",
    "        \"horizontal_resolution\": horizontal_resolution,\n",
    "        \"temporal_resolution\": temporal_resolution,\n",
    "        \"variable\": variables,\n",
    "        \"gcm_model\": gcm_model,\n",
    "        \"rcm_model\": rcm_model,\n",
    "        \"ensemble_member\": ensemble_member,\n",
    "        \"start_year\": [start_year],\n",
    "        \"end_year\": [end_year],\n",
    "        \"format\": \"zip\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        result = client.retrieve(dataset, request)\n",
    "        print(\"T√©l√©chargement termin√©. Sauvegarde du fichier...\")\n",
    "        path = result.download(filename)\n",
    "        print(f\"Fichier enregistr√© : {path}\")\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur pour {start_year}-{end_year} : {e}\")\n",
    "\n",
    "print(\"\\nTous les blocs RCP 4.5 ont √©t√© trait√©s avec succ√®s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3e872a-b1de-4672-b934-e14d900aef3b",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: 'Alice', serif; color: #2471A3; font-size: 2em; font-weight: 700;\">\n",
    "  √âtape 02 : Pr√©traitement des donn√©es\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93515d97-043d-4239-b728-0e6a2c03a056",
   "metadata": {},
   "source": [
    "> **Remarque**  \n",
    "> Pour les √©tapes de pr√©traitement (notamment l‚Äôextraction spatiale et la manipulation de fichiers CORDEX), nous basculerons vers le langage **R**, en utilisant notamment les packages [`eurocordexr`](https://github.com/Swiss-climate-initiative/eurocordexr) et [`nngeo`](https://cran.r-project.org/web/packages/nngeo/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e5c58-c846-47d2-bd6a-08a041165501",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: 'Alice', serif; color: #2471A3; font-size: 2em; font-weight: 700;\">\n",
    "  √âtape 03 : D√©biaisage des donn√©es\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecfacc3-aa0c-469a-9eed-c263f26d411e",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background-color: #ffffff;\n",
    "    padding: 25px 30px;\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 2px 2px 8px rgba(0,0,0,0.05);\n",
    "    font-family: 'Georgia', serif;\n",
    "    color: #2c2c2c;\n",
    "\">\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "Cette section r√©alise une <strong>correction de biais</strong> des projections climatiques √† l'aide de deux approches compl√©mentaires :\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px; line-height:1.6;\">\n",
    "  <li><strong>La m√©thode CDF-t</strong> (Cumulative Distribution Function transform), appliqu√©e ind√©pendamment √† chaque variable.</li>\n",
    "  <li><strong>La m√©thode dOTC</strong> (Optimal Transport Correction), appliqu√©e de fa√ßon <strong>multivari√©e</strong>, sur l'ensemble des variables simultan√©ment.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "L'objectif est de corriger les sorties quotidiennes des mod√®les climatiques (historiques et futurs) au niveau stationnel, en se basant sur les observations de r√©f√©rence issues de la r√©analyse SAFRAN.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "L'analyse concerne <strong>40 stations m√©t√©orologiques</strong> r√©parties sur le territoire fran√ßais. Les variables corrig√©es sont : les pr√©cipitations <code>pr</code>, le vent <code>sfcWind</code>, les temp√©ratures <code>tasmin</code> et <code>tasmax</code>, l'humidit√© <code>hurs</code>, le rayonnement <code>rsds</code> et l'√©vapotranspiration potentielle <code>ETP</code>.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:15px; margin-top:10px; line-height:1.6;\">\n",
    "Deux p√©riodes principales sont consid√©r√©es :\n",
    "</p>\n",
    "<ul style=\"font-size:15px; line-height:1.6;\">\n",
    "  <li><strong>P√©riode historique (1976‚Äì2005)</strong> : utilis√©e pour le calibrage √† partir des observations.</li>\n",
    "  <li><strong>P√©riode future (2006‚Äì2100)</strong> : corrig√©e par une <strong>correction mensuelle</strong> sur des <em>fen√™tres glissantes</em> de 30 ans, produisant chacune un noyau central de 10 ans corrig√©s.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; margin-top:10px; line-height:1.6;\">\n",
    "Pour chaque station, le script :\n",
    "</p>\n",
    "<ul style=\"font-size:15px; line-height:1.6;\">\n",
    "  <li>Charge les donn√©es d'observation, historiques et de projection future.</li>\n",
    "  <li>Applique la <strong>correction CDF-t</strong> (mensuelle, univari√©e).</li>\n",
    "  <li>Applique la <strong>correction dOTC</strong> (mensuelle, multivari√©e).</li>\n",
    "  <li>Applique un <strong>traitement SSR (Singularity Stochastic Removal)</strong> et une <strong>transformation log-exp</strong> pour la variable <code>pr</code>.</li>\n",
    "  <li>G√©n√®re :\n",
    "    <ul>\n",
    "      <li>Un fichier CSV par variable corrig√©e</li>\n",
    "      <li>Un fichier CSV fusionn√© par station contenant toutes les variables corrig√©es</li>\n",
    "      <li>Un journal de synth√®se des corrections</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; margin-top:10px; line-height:1.6;\">\n",
    "Le tableau suivant r√©sume les fen√™tres glissantes utilis√©es pour la correction :\n",
    "</p>\n",
    "\n",
    "<table style=\"\n",
    "    width: 100%;\n",
    "    border-collapse: collapse;\n",
    "    text-align: left;\n",
    "    font-size: 15px;\n",
    "    border: 1px solid #ddd;\n",
    "\">\n",
    "  <thead style=\"background-color: #fdeef4;\">\n",
    "    <tr>\n",
    "      <th style=\"padding: 10px; border: 1px solid #ddd;\">Fen√™tre</th>\n",
    "      <th style=\"padding: 10px; border: 1px solid #ddd;\">Fen√™tre de 30 ans</th>\n",
    "      <th style=\"padding: 10px; border: 1px solid #ddd;\">P√©riode corrig√©e</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr><td style=\"padding: 8px; border: 1px solid #ddd;\">Fen√™tre 1</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2006‚Äì2035</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2006‚Äì2025</td></tr>\n",
    "    <tr><td style=\"padding: 8px; border: 1px solid #ddd;\">Fen√™tre 2</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2016‚Äì2045</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2026‚Äì2035</td></tr>\n",
    "    <tr><td style=\"padding: 8px; border: 1px solid #ddd;\">Fen√™tre 3</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2026‚Äì2055</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2036‚Äì2045</td></tr>\n",
    "    <tr><td style=\"padding: 8px; border: 1px solid #ddd;\">Fen√™tre 4</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2036‚Äì2065</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2046‚Äì2055</td></tr>\n",
    "    <tr><td style=\"padding: 8px; border: 1px solid #ddd;\">Fen√™tre 5</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2046‚Äì2075</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2056‚Äì2065</td></tr>\n",
    "    <tr><td style=\"padding: 8px; border: 1px solid #ddd;\">Fen√™tre 6</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2056‚Äì2085</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2066‚Äì2075</td></tr>\n",
    "    <tr><td style=\"padding: 8px; border: 1px solid #ddd;\">Fen√™tre 7</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2066‚Äì2095</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2076‚Äì2085</td></tr>\n",
    "    <tr><td style=\"padding: 8px; border: 1px solid #ddd;\">Fen√™tre 8</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2076‚Äì2100</td><td style=\"padding: 8px; border: 1px solid #ddd;\">2086‚Äì2100</td></tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d6dc6-8312-479b-8d12-33929c14e537",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background-color: #ffffff;\n",
    "    padding: 25px 30px;\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 2px 2px 8px rgba(0,0,0,0.05);\n",
    "    font-family: 'Georgia', serif;\n",
    "    color: #2c2c2c;\n",
    "\">\n",
    "\n",
    "<h2 style=\"color:#c2185b; text-align:center; font-size:24px; margin-top:0;\">\n",
    "   1. Package de d√©biaisage SBCK\n",
    "</h2>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "Le package <strong>SBCK</strong> (<em>Statistical Bias Correction Kit</em>) est un outil Python open source d√©velopp√© sp√©cifiquement pour la correction de biais dans les projections climatiques.\n",
    "Il propose diff√©rentes m√©thodes de d√©biaisage avanc√©es, notamment&nbsp;:\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px; line-height:1.6;\">\n",
    "  <li><strong>CDF-t</strong> (<em>Cumulative Distribution Function transform</em>)‚ÄØ: correction univari√©e bas√©e sur l‚Äôajustement de la distribution des variables simul√©es sur celle de r√©f√©rence.</li>\n",
    "  <li><strong>dOTC</strong> (<em>distributional Optimal Transport Correction</em>)‚ÄØ: correction multivari√©e reposant sur la th√©orie du transport optimal, permettant de pr√©server la structure de d√©pendance entre variables climatiques.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "SBCK permet d‚Äôautomatiser les traitements, de comparer les m√©thodes et de g√©n√©rer facilement des jeux de donn√©es corrig√©s adapt√©s aux besoins de l‚Äôanalyse climatique.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "Pour plus d‚Äôinformations, consulter le d√©p√¥t GitHub&nbsp;: \n",
    "<a href=\"https://github.com/yrobink/SBCK\" target=\"_blank\">https://github.com/yrobink/SBCK</a>\n",
    "</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ab73c-3300-4941-92db-5fa4cdc4c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install SBCK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9b0ac-a5a1-493e-970b-25fe3448a199",
   "metadata": {},
   "source": [
    "<h2 style=\"\n",
    "    font-family: Georgia, serif;\n",
    "    font-size: 24px;\n",
    "    color: #c2185b;\n",
    "    margin-top: 40px;\n",
    "    text-align: center;\n",
    "\">\n",
    "  1. M√©thode CDF-t : Cumulative Distribution Function ‚Äì Transform\n",
    "</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a4e2e-bf28-4904-815e-be41c2d86ba0",
   "metadata": {},
   "source": [
    "<p style=\"\n",
    "    font-family: Georgia, serif;\n",
    "    font-size: 16px;\n",
    "    text-align: center;\n",
    "    color: #2c2c2c;\n",
    "\">\n",
    "  Ce script r√©alise une correction de biais g√©n√©ralis√©e pour deux mod√®les climatiques r√©gionaux : <strong>CERFACS-KNMIRACMO22</strong> et <strong>ECEARTH-KNMIRACMO22</strong>, selon le sc√©nario <em>RCP4.5</em>.  \n",
    "  Il applique la m√©thode <strong>CDF-t</strong> de mani√®re automatique sur toutes les stations disponibles, avec un traitement identique pour chaque mod√®le.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a70d4f9-8fcd-467e-9a4f-9c076275f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# PARAM√âTRAGE DES R√âPERTOIRES\n",
    "# ============================\n",
    "# - 'base' : chemin vers le dossier principal contenant les donn√©es extraites (c'est_√†-dire le dossier des fichiers csv apr√®s le pr√©-traitement des donn√©es avec R).\n",
    "# - 'models' : liste des mod√®les climatiques √† traiter.\n",
    "#   (ex : [\"CERFACS-KNMIRACMO22\", \"ECEARTH-KNMIRACMO22\"])\n",
    "# \n",
    "#  Adapter le chemin 'base' si besoin (par d√©faut : \"D:/cordex_data/projet_climat/extracted_csv\")\n",
    "# √† Modifier : Non de repertoire base , et  out_dir \n",
    "\n",
    "\n",
    "# R√©pertoires\n",
    "base = \"D:/cordex_data/projet_climat/extracted_csv\"  \n",
    "# Chemin vers le dossier principal contenant tous les fichiers CSV extraits apr√®s traitement avec \"eurocordexr\".\n",
    "\n",
    "models = [\"CERFACS-KNMIRACMO22\", \"ECEARTH-KNMIRACMO22\"]  \n",
    "# Liste des sous-dossiers dans 'extracted_csv', chaque sous-dossier correspondant √† un mod√®le climatique.\n",
    "# Exemple : le dossier 'extracted_csv' est divis√© en deux sous-dossiers, un par mod√®le.\n",
    "\n",
    "\n",
    "# le coode est inchang√© \n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from SBCK import CDFt             # M√©thode de correction univari√©e CDF-t du package SBCK\n",
    "from tqdm import tqdm             # Barre de progression\n",
    "\n",
    "np.random.seed(42)                # Fixer la graine al√©atoire pour la reproductibilit√©\n",
    "\n",
    "\n",
    "\n",
    "# Variables √† traiter\n",
    "variables_info = {\n",
    "    \"pr\": {\"obs_col\": \"pr\", \"hist_col\": \"pr\", \"rcp_col\": \"pr\", \"ssr\": True},                     # pr√©cipitations (avec SSR)\n",
    "    \"sfcWind\": {\"obs_col\": \"sfcWind\", \"hist_col\": \"sfcWind\", \"rcp_col\": \"sfcWind\", \"ssr\": False},\n",
    "    \"etpFAO\": {\"obs_col\": \"etp\", \"hist_col\": \"etp\", \"rcp_col\": \"etp\", \"ssr\": False},\n",
    "    \"rsds\": {\"obs_col\": \"rsds\", \"hist_col\": \"rsds\", \"rcp_col\": \"rsds\", \"ssr\": False},\n",
    "    \"hurs\": {\"obs_col\": \"hurs\", \"hist_col\": \"hurs\", \"rcp_col\": \"hurs\", \"ssr\": False},\n",
    "    \"tasmin\": {\"obs_col\": \"tasmin\", \"hist_col\": \"tasmin\", \"rcp_col\": \"tasmin\", \"ssr\": False},\n",
    "    \"tasmax\": {\"obs_col\": \"tasmax\", \"hist_col\": \"tasmax\", \"rcp_col\": \"tasmax\", \"ssr\": False},\n",
    "}\n",
    "# Transformations log-exp utilis√©es pour le traitement SSR (pour la variable = pr)\n",
    "def log1x(x): return np.where(x >= 1, x - 1, np.where((x > 0) & (x < 1), np.log(x), 0))\n",
    "def exp1x(x): return np.where(x < 0, np.exp(x), x + 1)\n",
    "\n",
    "# Fonction SSR : remplace les z√©ros par de petites valeurs al√©atoires dans l‚Äôintervalle ]0, th[\n",
    "def apply_ssr_precipitations_fixed_th(data_model, th):\n",
    "    \"\"\"\n",
    "    Applique une transformation SSR (Remplacement Stochastique de petites valeurs) √† une s√©rie temporelle de pr√©cipitations.\n",
    "\n",
    "    Objectif :\n",
    "        Remplacer les valeurs nulles par de tr√®s petites valeurs al√©atoires strictement positives,\n",
    "        tir√©es d‚Äôune distribution uniforme dans l‚Äôintervalle ]1e-6, th[.\n",
    "        Cela permet d‚Äô√©viter les probl√®mes li√©s √† la transformation logarithmique,\n",
    "        puisque log(0) est ind√©fini.\n",
    "\n",
    "    Param√®tres :\n",
    "        data_model (array-like) : tableau numpy contenant les donn√©es de pr√©cipitation.\n",
    "        th (float) : seuil positif fixe, g√©n√©ralement d√©fini comme le minimum de toutes les valeurs strictement positives\n",
    "                     dans les donn√©es du mod√®le et les observations de r√©f√©rence.\n",
    "\n",
    "    Retour :\n",
    "        data_corr (np.ndarray) : tableau corrig√© dans lequel toutes les valeurs nulles ont √©t√© remplac√©es par des valeurs al√©atoires.\n",
    "    \"\"\"\n",
    "    data_corr = data_model.copy()               # Copier la s√©rie pour ne pas modifier les donn√©es d'origine\n",
    "    zero_idx = np.where(data_corr == 0)[0]      # Identifier les indices o√π la valeur est z√©ro (pr√©cipitations nulles)\n",
    "    if len(zero_idx) > 0:                       # V√©rifier s‚Äôil y a des z√©ros ; si oui, appliquer le SSR\n",
    "        random_values = np.random.uniform(low=1e-6, high=th, size=len(zero_idx))    \n",
    "        # np.random.uniform(low, high, size) :\n",
    "        # - low : borne inf√©rieure (incluse)\n",
    "        # - high : borne sup√©rieure (exclue)\n",
    "        # - size : nombre de valeurs al√©atoires √† g√©n√©rer\n",
    "        data_corr[zero_idx] = random_values    # Remplacer tous les z√©ros par les valeurs al√©atoires g√©n√©r√©es\n",
    "    \n",
    "    return data_corr\n",
    "\n",
    "# Fen√™tres glissantes\n",
    "# D√©finition des fen√™tres glissantes (30 ans, noyau central de 10 ans)\n",
    "windows = [\n",
    "    (2006, 2035, 2006, 2025),\n",
    "    (2016, 2045, 2026, 2035),\n",
    "    (2026, 2055, 2036, 2045),\n",
    "    (2036, 2065, 2046, 2055),\n",
    "    (2046, 2075, 2056, 2065),\n",
    "    (2056, 2085, 2066, 2075),\n",
    "    (2066, 2095, 2076, 2085),\n",
    "    (2076, 2100, 2086, 2100),\n",
    "]\n",
    "\n",
    "\n",
    "# Boucle sur les deux mod√®les climatiques √† traiter (ex. : CERFACS-KNMIRACMO22, ECEARTH-KNMIRACMO22)\n",
    "for model in models:\n",
    "    # D√©finition des chemins vers les fichiers historiques, futurs et observations SAFRAN    \n",
    "    hist_dir = os.path.join(base, \"stations_hist\", model)   # Donn√©es historiques du mod√®le\n",
    "    fut_dir = os.path.join(base, \"stations_fut\", model)     # Donn√©es futures (RCP4.5) du mod√®le\n",
    "    obs_dir = os.path.join(base, \"stations_SAFRAN\")         # Donn√©es d'observation SAFRAN\n",
    "    out_dir = os.path.join(\"D:/cordex_data/projet_climat/debiaised\", f\"{model}-CDFT\")     # Dossier de sortie      # √† VOIR\n",
    "    os.makedirs(out_dir, exist_ok=True)                                                   # Cr√©er le dossier s'il n'existe pas\n",
    " \n",
    "    print(f\"\\n=== Traitement du mod√®le : {model} ===\")\n",
    "    log_list = []          # Liste pour enregistrer les logs (succ√®s ou erreurs)\n",
    "    \n",
    "    # Boucle sur chaque station du mod√®le\n",
    "    for hist_file in tqdm(os.listdir(hist_dir)):\n",
    "        # Ignorer les fichiers qui ne sont pas du type \"-hist.csv\"\n",
    "        if not hist_file.endswith(\"-hist.csv\"):\n",
    "            continue\n",
    "        # Extraction du nom de la station √† partir du nom de fichier\n",
    "        station = hist_file.split(f\"-{model}-hist.csv\")[0]\n",
    "        # Construction des chemins vers les fichiers de la station\n",
    "        hist_path = os.path.join(hist_dir, hist_file)\n",
    "        rcp_path = os.path.join(fut_dir, f\"{station}-{model}-rcp45.csv\")\n",
    "        obs_path = os.path.join(obs_dir, f\"{station}_SafranDrias.csv\")\n",
    "\n",
    "        # V√©rification de l‚Äôexistence des fichiers RCP et SAFRAN\n",
    "        if not os.path.exists(rcp_path):\n",
    "            log_list.append((station, \"ERROR\", f\"Fichier RCP manquant : {rcp_path}\"))\n",
    "            continue\n",
    "        if not os.path.exists(obs_path):\n",
    "            log_list.append((station, \"ERROR\", f\"Fichier obs SAFRAN manquant : {obs_path}\"))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Chargement des fichiers CSV\n",
    "            hist = pd.read_csv(hist_path, sep=\";\")\n",
    "            rcp = pd.read_csv(rcp_path, sep=\";\")\n",
    "            obs = pd.read_csv(obs_path, sep=\";\")\n",
    "\n",
    "            # Conversion des colonnes \"date\" en format datetime\n",
    "            for df in [hist, rcp, obs]:\n",
    "                df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "            # S√©lection des p√©riodes  \n",
    "            Y0_base = obs[(obs[\"date\"] >= \"1976-01-01\") & (obs[\"date\"] <= \"2005-12-31\")].copy()   # observations de r√©f√©rence\n",
    "            X0_base = hist[(hist[\"date\"] >= \"1976-01-01\") & (hist[\"date\"] <= \"2005-12-31\")].copy() # mod√®le historique\n",
    "            X1_base = rcp[rcp[\"date\"] > \"2005-12-31\"].copy()      # projections futures\n",
    "\n",
    "            results_all_vars = {}       # Dictionnaire final des r√©sultats corrig√©s pour chaque variable\n",
    "            \n",
    "            # Boucle sur les variables climatiques √† corriger\n",
    "            for var, info in variables_info.items():\n",
    "                resultats_Z0 = Y0_base[[\"date\"]].copy()      # Pr√©paration des r√©sultats historiques corrig√©s\n",
    "                resultats_Z0[f\"{var}_corr\"] = np.nan\n",
    "                results_all = {}                             # R√©sultats pour toutes les fen√™tres glissantes\n",
    "                \n",
    "                # Traitement sp√©cifique pour les pr√©cipitations (SSR + log-exp)\n",
    "                if info[\"ssr\"]:\n",
    "                    all_pos_values = np.concatenate([\n",
    "                        X0_base[info[\"hist_col\"]][X0_base[info[\"hist_col\"]] > 0],   # valeurs > 0 dans les donn√©es historiques\n",
    "                        X1_base[info[\"rcp_col\"]][X1_base[info[\"rcp_col\"]] > 0],     # valeurs > 0 dans les projections futures\n",
    "                        Y0_base[info[\"obs_col\"]][Y0_base[info[\"obs_col\"]] > 0],     # valeurs > 0 dans les observations\n",
    "                    ])\n",
    "                    th_global = all_pos_values.min()    # Seuil SSR : d√©fini comme la plus petite valeur strictement positive parmi les donn√©es du mod√®le et les observations\n",
    "                    \n",
    "                    # Application de la transformation SSR sur les donn√©es\n",
    "                    X0_base[info[\"hist_col\"]] = apply_ssr_precipitations_fixed_th(X0_base[info[\"hist_col\"]].values, th_global) \n",
    "                    Y0_base[info[\"obs_col\"]] = apply_ssr_precipitations_fixed_th(Y0_base[info[\"obs_col\"]].values, th_global)\n",
    "                    X1_base[info[\"rcp_col\"]] = apply_ssr_precipitations_fixed_th(X1_base[info[\"rcp_col\"]].values, th_global)\n",
    "                    \n",
    "                # Correction de biais mensuelle par CDF-t avec fen√™tres glissantes\n",
    "                for full_start, full_end, corr_start, corr_end in windows:\n",
    "                    X1_full = X1_base[(X1_base[\"date\"] >= f\"{full_start}-01-01\") & (X1_base[\"date\"] <= f\"{full_end}-12-31\")]  # Extraire la fen√™tre compl√®te de 30 ans de projections futures (X1) utilis√©e pour la correction de biais\n",
    "                    X1_corr = X1_full[(X1_full[\"date\"] >= f\"{corr_start}-01-01\") & (X1_full[\"date\"] <= f\"{corr_end}-12-31\")]  # Extraire la sous-p√©riode centrale (X1_corr) √† l‚Äôint√©rieur de cette fen√™tre, sur laquelle la correction sera effectivement appliqu√©e\n",
    "                    if X1_corr.empty:\n",
    "                        print(\" Aucune donn√©e pour cette fen√™tre.\")     # Optionnel : v√©rifier que la fen√™tre contient bien des donn√©es avant de continuer :)\n",
    "                        continue\n",
    "\n",
    "                    resultats = X1_corr[[\"date\"]].copy()   # Cr√©er un DataFrame contenant uniquement la colonne \"date\" pour la p√©riode future √† corriger\n",
    "                    resultats[f\"{var}_corr\"] = np.nan     # Ajouter une nouvelle colonne (ex. : \"pr_corr\") remplie de valeurs manquantes (NaN) comme emplacement r√©serv√©\n",
    "\n",
    "                    for m in range(1, 13):  # Pour chaque mois de janvier √† d√©cembre\n",
    "                        y0_data = Y0_base[Y0_base[\"date\"].dt.month == m][info[\"obs_col\"]].values  # Donn√©es observ√©es (SAFRAN), corrig√©es par SSR, pour le mois m\n",
    "                        x0_data = X0_base[X0_base[\"date\"].dt.month == m][info[\"hist_col\"]].values # Donn√©es du mod√®le historique, corrig√©es par SSR, pour le mois m\n",
    "                        x1_data = X1_corr[X1_corr[\"date\"].dt.month == m][info[\"rcp_col\"]].values  # Donn√©es du mod√®le futur (RCP), corrig√©es par SSR, pour le mois m\n",
    "                        \n",
    "                        # Traitement sp√©cifique pour les pr√©cipitations (SSR + log-exp)\n",
    "                        if info[\"ssr\"]:  # Si le SSR est activ√©, c‚Äôest-√†-dire que la variable est la pr√©cipitation (\"pr\")\n",
    "                         # Appliquer la transformation log1x √† y0_data, x0_data et x1_data\n",
    "                         # Ensuite, remodeler les tableaux 1D en matrices 2D (N lignes, 1 colonne)\n",
    "                         # Ce format est requis par des m√©thodes statistiques comme CDF-t, qui attendent des entr√©es en 2D\n",
    "                            y0, x0, x1 = log1x(y0_data).reshape(-1, 1), log1x(x0_data).reshape(-1, 1), log1x(x1_data).reshape(-1, 1)     # -1 permet √† NumPy de d√©terminer automatiquement le nombre de lignes ; 1 fixe le nombre de colonnes (une seule variable)\n",
    "                            cdft = CDFt()                   # Initialisation du mod√®le CDF-t\n",
    "                            cdft.fit(y0, x0, x1)\n",
    "                            Z1, Z0 = cdft.predict(x1, x0)   # Z1 corresponds to the bias-corrected future data, Z0 corresponds to the bias-corrected historical data\n",
    "                            Z_corr, Z_corr_hist = exp1x(Z1.flatten()), exp1x(Z0.flatten()) # Apply the inverse transformation (exp1x) to the corrected future data (same for historical data)\n",
    "                            Z_corr[Z_corr < th_global] = 0       # Reset corrected future values below the global SSR threshold back to zero\n",
    "                            Z_corr_hist[Z_corr_hist < th_global] = 0  # Same reset applied to corrected historical values below the SSR threshold\n",
    "                        else:  # for all variables expect precipitation\n",
    "                            y0, x0, x1 = y0_data.reshape(-1, 1), x0_data.reshape(-1, 1), x1_data.reshape(-1, 1)\n",
    "                            cdft = CDFt()\n",
    "                            cdft.fit(y0, x0, x1)\n",
    "                            Z1, Z0 = cdft.predict(x1, x0)\n",
    "                            Z_corr, Z_corr_hist = Z1.flatten(), Z0.flatten()\n",
    "\n",
    "                        mask_x1 = X1_corr[\"date\"].dt.month == m # Masque pour s√©lectionner les dates du mois m dans les donn√©es futures corrig√©es\n",
    "                        mask_y0 = Y0_base[\"date\"].dt.month == m # Masque pour s√©lectionner les dates du mois m dans les observations historiques\n",
    "                        resultats.loc[mask_x1, f\"{var}_corr\"] = np.round(Z_corr[:mask_x1.sum()], 1)  # Ins√©rer les valeurs corrig√©es futures (Z_corr), arrondies √† 0.1, dans le DataFrame des r√©sultats futurs\n",
    "                        resultats_Z0.loc[mask_y0, f\"{var}_corr\"] = np.round(Z_corr_hist[:mask_y0.sum()], 1) # Ins√©rer les valeurs corrig√©es historiques (Z_corr_hist), arrondies √† 0.1, dans le DataFrame des r√©sultats historiques\n",
    "\n",
    "                    results_all[f\"{corr_start}_{corr_end}\"] = resultats # Sauvegarder les r√©sultats de la p√©riode corrig√©e (ex. : \"2006_2025\") dans le dictionnaire des fen√™tres glissantes\n",
    "\n",
    "                results_all[\"historique_corrige\"] = resultats_Z0   # Ajouter les r√©sultats corrig√©s sur la p√©riode historique dans le dictionnaire des r√©sultats (cl√© sp√©ciale : \"historique_corrige\")\n",
    "                # Concat√©ner tous les r√©sultats (fen√™tres glissantes + historique corrig√©) en un seul DataFrame\n",
    "                # R√©initialiser les index et trier les lignes par date\n",
    "                final_var = pd.concat(results_all.values(), ignore_index=True).sort_values(\"date\")\n",
    "                \n",
    "                # Appliquer des contraintes physiques\n",
    "                if 'hurs_corr' in final_var.columns:\n",
    "                    final_var.loc[final_var['hurs_corr'] > 100, 'hurs_corr'] = 100   # L‚Äôhumidit√© relative (hurs) ne peut pas d√©passer 100 %, on tronque √† 100 si besoin\n",
    "                for v in ['rsds_corr', 'etpFAO_corr', 'sfcWind_corr', 'hurs_corr']:\n",
    "                    if v in final_var.columns:\n",
    "                        final_var.loc[final_var[v] < 0, v] = 0\n",
    "\n",
    "                results_all_vars[var] = final_var\n",
    "\n",
    "            df_final = pd.DataFrame({\"date\": pd.date_range(\"1976-01-01\", \"2100-12-31\", freq=\"D\")})\n",
    "            for var, df_var in results_all_vars.items():\n",
    "                df_final = df_final.merge(df_var[[\"date\", f\"{var}_corr\"]], on=\"date\", how=\"left\")\n",
    "                \n",
    "            # id = id DRIAS,  grid lon : obs and grid lat : Obs \n",
    "            id_station = obs.get(\"id\", [None])[0]\n",
    "            nom_station = obs.get(\"nom\", [station])[0]\n",
    "            grid_lon = obs.get(\"grid_lon\", [None])[0]\n",
    "            grid_lat = obs.get(\"grid_lat\", [None])[0]\n",
    "\n",
    "            df_final.insert(0, \"id\", id_station)\n",
    "            df_final.insert(1, \"nom\", nom_station)\n",
    "            df_final.insert(2, \"grid_lon\", grid_lon)\n",
    "            df_final.insert(3, \"grid_lat\", grid_lat)\n",
    "\n",
    "            df_final.to_csv(f\"{out_dir}/CDFT_all_vars_{station}_{model}_rcp45_1976_2100.csv\", sep=\";\", index=False)\n",
    "            log_list.append((station, \"OK\", \"\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            log_list.append((station, \"ERROR\", str(e)))\n",
    "\n",
    "    pd.DataFrame(log_list, columns=[\"station\", \"status\", \"message\"]).to_csv(\n",
    "        os.path.join(out_dir, \"cdf_correction_log.csv\"), sep=\";\", index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8511c344-5fc6-4068-8287-e65b9aaa4629",
   "metadata": {},
   "source": [
    "<h2 style=\"\n",
    "    font-family: Georgia, serif;\n",
    "    font-size: 24px;\n",
    "    color: #c2185b;\n",
    "    margin-top: 40px;\n",
    "    text-align: center;\n",
    "\">\n",
    "  2. M√©thode dOTC : dynamical Optimal Transport Correction\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d6e49e-cccd-4c52-a155-100aa4ec5ae8",
   "metadata": {},
   "source": [
    "<p style=\"\n",
    "    font-family: Georgia, serif;\n",
    "    font-size: 16px;\n",
    "    text-align: center;\n",
    "    color: #2c2c2c;\n",
    "\">\n",
    "  Ce script r√©alise une correction de biais g√©n√©ralis√©e pour deux mod√®les climatiques r√©gionaux : <strong>CERFACS-KNMIRACMO22</strong> et <strong>ECEARTH-KNMIRACMO22</strong>, selon le sc√©nario <em>RCP4.5</em>.  \n",
    "  Il applique la m√©thode <strong>dOTC</strong> de mani√®re automatique sur toutes les stations disponibles, avec un traitement identique pour chaque mod√®le.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a9153-2414-475e-af12-9ed50b7a1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from SBCK import dOTC\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "variables_info = {\n",
    "    \"pr\": {\"obs_col\": \"pr\", \"hist_col\": \"pr\", \"rcp_col\": \"pr\", \"ssr\": True},\n",
    "    \"sfcWind\": {\"obs_col\": \"sfcWind\", \"hist_col\": \"sfcWind\", \"rcp_col\": \"sfcWind\", \"ssr\": False},\n",
    "    \"etp\": {\"obs_col\": \"etp\", \"hist_col\": \"etp\", \"rcp_col\": \"etp\", \"ssr\": False},\n",
    "    \"rsds\": {\"obs_col\": \"rsds\", \"hist_col\": \"rsds\", \"rcp_col\": \"rsds\", \"ssr\": False},\n",
    "    \"hurs\": {\"obs_col\": \"hurs\", \"hist_col\": \"hurs\", \"rcp_col\": \"hurs\", \"ssr\": False},\n",
    "    \"tasmin\": {\"obs_col\": \"tasmin\", \"hist_col\": \"tasmin\", \"rcp_col\": \"tasmin\", \"ssr\": False},\n",
    "    \"tasmax\": {\"obs_col\": \"tasmax\", \"hist_col\": \"tasmax\", \"rcp_col\": \"tasmax\", \"ssr\": False},\n",
    "}\n",
    "\n",
    "vars = list(variables_info.keys())\n",
    "vars_non_negatives = [\"sfcWind\", \"etp\", \"rsds\", \"hurs\"]\n",
    "models = [\"CERFACS-KNMIRACMO22\", \"ECEARTH-KNMIRACMO22\"]\n",
    "\n",
    "def apply_ssr_precipitations_fixed_th(data_model, th):\n",
    "    data_corr = data_model.copy()\n",
    "    zero_idx = np.where(data_corr == 0)[0]\n",
    "    if len(zero_idx) > 0:\n",
    "        random_values = np.random.uniform(low=1e-6, high=th, size=len(zero_idx))\n",
    "        data_corr[zero_idx] = random_values\n",
    "    return data_corr\n",
    "\n",
    "def log1x(x): return np.where(x >= 1, x - 1, np.where((x > 0) & (x < 1), np.log(x), 0))\n",
    "def exp1x(x): return np.where(x < 0, np.exp(x), x + 1)\n",
    "\n",
    "windows = [\n",
    "    (2006, 2035, 2006, 2025), (2016, 2045, 2026, 2035),\n",
    "    (2026, 2055, 2036, 2045), (2036, 2065, 2046, 2055),\n",
    "    (2046, 2075, 2056, 2065), (2056, 2085, 2066, 2075),\n",
    "    (2066, 2095, 2076, 2085), (2076, 2100, 2086, 2100)\n",
    "]\n",
    "\n",
    "base = \"D:/cordex_data/projet_climat/extracted_csv\"\n",
    "out_root = \"D:/cordex_data/projet_climat/debiaised\"\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\n=== Traitement du mod√®le : {model} ===\")\n",
    "    hist_dir = os.path.join(base, \"stations_hist\", model)\n",
    "    rcp_dir = os.path.join(base, \"stations_fut\", model)\n",
    "    obs_dir = os.path.join(base, \"stations_SAFRAN\")\n",
    "    out_dir = os.path.join(out_root, f\"{model}-dOTC\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    log_list = []\n",
    "    for hist_file in tqdm(os.listdir(hist_dir)):\n",
    "        if not hist_file.endswith(\"-hist.csv\"):\n",
    "            continue\n",
    "        station = hist_file.replace(f\"-{model}-hist.csv\", \"\")\n",
    "        hist_path = os.path.join(hist_dir, hist_file)\n",
    "        rcp_path = os.path.join(rcp_dir, f\"{station}-{model}-rcp45.csv\")\n",
    "        obs_path = os.path.join(obs_dir, f\"{station}_SafranDrias.csv\")\n",
    "\n",
    "        if not (os.path.exists(hist_path) and os.path.exists(rcp_path) and os.path.exists(obs_path)):\n",
    "            log_list.append((station, \"ERROR\", \"Fichiers manquants\"))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            obs = pd.read_csv(obs_path, sep=\";\", parse_dates=[\"date\"])\n",
    "            hist = pd.read_csv(hist_path, sep=\";\", parse_dates=[\"date\"], dayfirst=True)\n",
    "            rcp = pd.read_csv(rcp_path, sep=\";\", parse_dates=[\"date\"], dayfirst=True)\n",
    "            for df in [obs, hist, rcp]:\n",
    "                df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "            Y0 = obs[(obs[\"date\"] >= \"1976-01-01\") & (obs[\"date\"] <= \"2005-12-31\")].copy()\n",
    "            X0 = hist[(hist[\"date\"] >= \"1976-01-01\") & (hist[\"date\"] <= \"2005-12-31\")].copy()\n",
    "            X1 = rcp[rcp[\"date\"] > \"2005-12-31\"].copy()\n",
    "\n",
    "            resultats_Z0 = pd.DataFrame({\"date\": Y0[\"date\"]})\n",
    "            resultats = pd.DataFrame({\"date\": X1[\"date\"]})\n",
    "            results_all = {}\n",
    "\n",
    "            for full_start, full_end, corr_start, corr_end in windows:\n",
    "                X1_full = X1[(X1[\"date\"] >= f\"{full_start}-01-01\") & (X1[\"date\"] <= f\"{full_end}-12-31\")].copy()\n",
    "                X1_corr = X1_full[(X1_full[\"date\"] >= f\"{corr_start}-01-01\") & (X1_full[\"date\"] <= f\"{corr_end}-12-31\")].copy()\n",
    "\n",
    "                resultats_window = pd.DataFrame({\"date\": X1_corr[\"date\"]})\n",
    "\n",
    "                for var in vars:\n",
    "                    resultats_window[f\"{var}_corr\"] = np.nan\n",
    "                    resultats_Z0[f\"{var}_corr\"] = np.nan\n",
    "\n",
    "                for m in range(1, 13):\n",
    "                    y0_mois = Y0[Y0[\"date\"].dt.month == m][vars].copy()\n",
    "                    x0_mois = X0[X0[\"date\"].dt.month == m][vars].copy()\n",
    "                    x1_mois = X1_full[X1_full[\"date\"].dt.month == m][vars + [\"date\"]].copy()\n",
    "                    x1_corr_mois = X1_corr[X1_corr[\"date\"].dt.month == m][vars + [\"date\"]].copy()\n",
    "\n",
    "                    if len(x1_corr_mois) == 0:\n",
    "                        continue\n",
    "\n",
    "                    if \"pr\" in vars:\n",
    "                        all_pos_pr = np.concatenate([\n",
    "                            x0_mois[\"pr\"][x0_mois[\"pr\"] > 0],\n",
    "                            y0_mois[\"pr\"][y0_mois[\"pr\"] > 0],\n",
    "                            x1_mois[\"pr\"][x1_mois[\"pr\"] > 0]\n",
    "                        ])\n",
    "                        th_global = max(np.min(all_pos_pr), 1e-6)\n",
    "                        y0_mois[\"pr\"] = log1x(apply_ssr_precipitations_fixed_th(y0_mois[\"pr\"].values, th_global))\n",
    "                        x0_mois[\"pr\"] = log1x(apply_ssr_precipitations_fixed_th(x0_mois[\"pr\"].values, th_global))\n",
    "                        x1_mois[\"pr\"] = log1x(apply_ssr_precipitations_fixed_th(x1_mois[\"pr\"].values, th_global))\n",
    "\n",
    "                    model_d = dOTC()\n",
    "                    model_d.fit(y0_mois.values, x0_mois.values, x1_corr_mois[vars].values)\n",
    "                    Z_corr, Z_corr_hist = model_d.predict(x1_corr_mois[vars].values, x0_mois.values)\n",
    "\n",
    "                    for j, var in enumerate(vars):\n",
    "                        mask_x1 = x1_corr_mois[\"date\"].dt.month == m\n",
    "                        mask_y0 = Y0[Y0[\"date\"].dt.month == m].index\n",
    "\n",
    "                        z_corr_hist = Z_corr_hist[:, j]\n",
    "                        z_corr = Z_corr[:, j]\n",
    "\n",
    "                        if var == \"pr\":\n",
    "                            z_corr = exp1x(z_corr)\n",
    "                            z_corr_hist = exp1x(z_corr_hist)\n",
    "                            z_corr[z_corr < th_global] = 0\n",
    "                            z_corr_hist[z_corr_hist < th_global] = 0\n",
    "\n",
    "                        if var in vars_non_negatives:\n",
    "                            z_corr[z_corr < 0] = 0\n",
    "                            z_corr_hist[z_corr_hist < 0] = 0\n",
    "                        if var == \"hurs\":\n",
    "                            z_corr[z_corr > 100] = 100\n",
    "                            z_corr_hist[z_corr_hist > 100] = 100\n",
    "\n",
    "                        resultats_window.loc[x1_corr_mois.index, f\"{var}_corr\"] = np.round(z_corr, 1)\n",
    "                        resultats_Z0.loc[mask_y0, f\"{var}_corr\"] = np.round(z_corr_hist[:len(mask_y0)], 1)\n",
    "\n",
    "                results_all[f\"{corr_start}_{corr_end}\"] = resultats_window\n",
    "\n",
    "            results_all[\"historique_corrige\"] = resultats_Z0\n",
    "\n",
    "            df_final = pd.DataFrame({\"date\": pd.date_range(\"1976-01-01\", \"2100-12-31\", freq=\"D\")})\n",
    "            for var in vars:\n",
    "                # Exclure explicitement la cl√© \"historique_corrige\" pour √©viter de doubler\n",
    "                temp_values = [df for k, df in results_all.items() if k != \"historique_corrige\"]\n",
    "                df_all = pd.concat([results_all[\"historique_corrige\"], *temp_values], ignore_index=True)\n",
    "                df_var = df_all[[\"date\", f\"{var}_corr\"]].dropna().drop_duplicates(\"date\").sort_values(\"date\")\n",
    "                df_final = df_final.merge(df_var, on=\"date\", how=\"left\")\n",
    "\n",
    "            id_station = obs.get(\"id\", [None])[0] if \"id\" in obs.columns else station\n",
    "            nom_station = obs.get(\"nom\", [station])[0] if \"nom\" in obs.columns else station\n",
    "            grid_lon = obs.get(\"grid_lon\", [None])[0] if \"grid_lon\" in obs.columns else None\n",
    "            grid_lat = obs.get(\"grid_lat\", [None])[0] if \"grid_lat\" in obs.columns else None\n",
    "\n",
    "            df_final.insert(0, \"id\", id_station)\n",
    "            df_final.insert(1, \"nom\", nom_station)\n",
    "            df_final.insert(2, \"grid_lon\", grid_lon)\n",
    "            df_final.insert(3, \"grid_lat\", grid_lat)\n",
    "\n",
    "            out_file = os.path.join(out_dir, f\"dOTC_all_vars_{station}_{model}_rcp45_1976_2100.csv\")\n",
    "            df_final.to_csv(out_file, sep=\";\", index=False)\n",
    "            log_list.append((station, \"OK\", \"\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            log_list.append((station, \"ERROR\", str(e)))\n",
    "\n",
    "    pd.DataFrame(log_list, columns=[\"station\", \"status\", \"message\"]).to_csv(\n",
    "        os.path.join(out_dir, \"dOTC_log.csv\"), sep=\";\", index=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
